\chapter{Results and Evaluation}\label{s:results}

\section{Experiment setup}

The collection process was executed over a week. We retrieved information from
172 real CI jobs from the pipeline under study. We submitted two coverage-collecting jobs 
to the build system and collected the coverage traces.

The scale of the collected information limited the number of jobs we were able to
collect and process.\footnote{The information on CI job executions amounted to 6GB of data and 
the coverage data to 44GB. }

Each of the datasets considered: \texttt{EP-NCI}, \texttt{EP-CI}, \texttt{CP-NCI}, and \texttt{CP-CI} 
consisted of 172 queries corresponding to the collected CI jobs, from which 20\% 
of the failing test jobs were used as the validation dataset. Each of these jobs
contains around 18000 tests.

It is worth comparing the size of this dataset with the provided datasets from the literature.
Spieker et. al. in \cite{DBLP:journals/corr/abs-1811-04122} use the \emph{Paint Control} dataset consisting of 180 cycles
with an average of 65 tests each.  In \cite{Bertolino2020LearningtoRankVR}, Bertolino, et. al. consider datasets
of 522 cycles with an average of 22 tests each, which they created from the Apache Commons projects 
commit history.

The training of the different \emph{Learning to Rank} algorithms was performed with the
different datasets and varying criteria. For implementation, we used Ranklib 2.17. The training
was performed with the High-Performance Cluster from the IT University of Copenhagen. The minimum resources 
required for each of the training jobs were 115GB of RAM, which resulted in a training time with an upper limit of 2 hours
for nodes with processors with over 32 cores.

Using the trained models and obtaining the evaluation metrics was lightweight to perform on
a regular computer. This negligible process would be the overhead that the CI pipeline would have to execute
to obtain the proposed selections of these approaches.

\section{Overview of the results presented}
First, we use the NAPFD metric as defined in section \ref{sec:bg-metrics-tsp} to compare the effectiveness of the 
rankings for the Test Prioritization problem.

For each ranking algorithm, we present how the different criteria used influence the
NAPFD behavior. To emphasize, the varying criteria for the experiments were:
\begin{itemize}
    \item Priority function used to label the training dataset: either the exponential prioritization proposed by Bertolino, et. al. in \cite{Bertolino2020LearningtoRankVR}, or the discrete coverage prioritization we proposed.
    \item Features characterizing each test run: we included features related to AL file changes, test execution history, and varied either considering coverage properties or not.
    \item Training metric used for training the ranking algorithms: we use the different ranking evaluation metrics explained in section \ref{sec:bg-metrics-tsp}.
    \item For MART and LambdaMART the number of regression trees used: as explained in section \ref{s:bg-tsp-ltr-algs}, these methods combine several regression trees as weak learners. We use 5, 10, 20, and 30 trees in our experiments.
\end{itemize}

For each algorithm, we present box plots of the distribution of this metric for some of these configurations. 

Additionally, for each of these ranking algorithms and configurations, we induce selection algorithms by taking the tests
for which the model predicted the highest priorities. To determine the number of tests that we take
from a prioritization we use the following criteria:

\begin{itemize}
    \item A strictly safe selection \texttt{S-SEL}: we take tests from the prioritization until for all the evaluated jobs, every test failure is included.
    \item An above 80\% average selection \texttt{80-SEL}: we take tests from the prioritization until the average of their \emph{inclusiveness} is at least 80\%.
    \item An above 50\% average selection \texttt{50-SEL}: we take tests from the prioritization until the average of their \emph{inclusiveness} is at least 50\%.
\end{itemize}

These selections are considered to support the research question \textbf{RQ2}, to obtain
concrete quantities of the number of tests that could be omitted from execution
in this pipeline.

For a complete set of values shown in the distribution plots and more evaluation metrics, see appendix \ref{sec:appendix-evaluation-results}
and the evaluation data provided in the accompanying repository.

\section{Results for Coordinate Ascent}

Using different training metrics did not have a large impact on the behavior of this algorithm. 
However, the highest average and lowest variance of the NAPFD of different datasets ranked was obtained with the 
\texttt{NDCG@30} training metric.

Additionally, results show that using coverage information as part of the features characterizing a change
results in lower values of NAPFD. However, for the coverage priority function, the NAPFD values were 
consistently higher.

The best results for this algorithm, across the different metrics, were obtained by datasets that do not
use coverage information in the features, but that use the coverage prioritization we proposed. In the worst case
for these datasets, the NAPFD of the proposed test ranking can be as low as 67\%, but as high as 99\%.

Figures \ref{fig:coordinate-ascent-02-napfd} to \ref{fig:coordinate-ascent-06-napfd} show the box plot of the distribution
of NAPFD values for the jobs in the validation dataset for the different training metrics. 

Additionally, for \texttt{NDCG@30}, we present the distribution of values 
across each different dataset of the \emph{time to the first failure} metric in figure \ref{fig:coordinate-ascent-06-tff}. 

We observe that only in the case of the dataset with coverage information and coverage prioritization, the proposed prioritization
achieves a value as high as 88\% of the total execution time with an average of 14\%. For the remaining datasets, the first failure is detected
at most in the first 2\% of the total execution time.

Regarding the induced selections, for the training metric \texttt{NDCG@30} and \texttt{CP-CI} dataset the following results were obtained:
\begin{itemize}
    \item \texttt{S-SEL}: A safe selection was achieved with a selection size of 40\%, corresponding to 10\% of the execution time.
    \item \texttt{80-SEL}: A selection with average inclusiveness over 80\% was achieved with a selection size of 40\%, corresponding to 10\% of the execution time.
    \item \texttt{50-SEL}: A selection with average inclusiveness over 50\% was achieved with a selection size of 10\%, corresponding to 3\% of the execution time.
\end{itemize}

\begin{figure}
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:coordinate-ascent-02-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/coordinateascent-02/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the Coordinate Ascent algorithm using the \texttt{DCG@10} training metric.}}
    \end{minipage}%
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:coordinate-ascent-03-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/coordinateascent-03/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the Coordinate Ascent algorithm using the \texttt{MAP} training metric.}}
    \end{minipage}%
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:coordinate-ascent-01-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/coordinateascent-01/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the Coordinate Ascent algorithm using the \texttt{NDCG@10} training metric.}}
    \end{minipage}%
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:coordinate-ascent-04-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/coordinateascent-04/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the Coordinate Ascent algorithm using the \texttt{NDCG@20} training metric.}}
    \end{minipage}%
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:coordinate-ascent-06-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/coordinateascent-06/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the Coordinate Ascent algorithm using the \texttt{NDCG@30} training metric.}}
    \end{minipage}%
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:coordinate-ascent-06-tff}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/coordinateascent-06/distribution-comparison-TimeToFirstFailure.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of times to first failure for the Coordinate Ascent algorithm using the \texttt{NDCG@30} training metric.}}
    \end{minipage}%
\end{figure}

\section{Results for LambdaMART}
For LambdaMART, using the \texttt{MAP} metric, consistently ranked with a NAPFD value of 66\% for any of the different 
trained datasets, as it can be seen in figure \ref{fig:lambdamart-13-napfd}. With the \texttt{ERR@10} metric, the training did not converge, resulting in an invalid model.

Apart from these two metrics, the other training metrics behaved similarly across the different datasets. The best NAPFD
value was obtained with the \texttt{DCG@10} metric.

Regarding the number of trees used for gradient boosting, the best performing values were obtained with 20 regression trees.

For this algorithm, using the coverage prioritization proposed yielded better NAPFD values. When using the exponential prioritization,
using coverage information as features increased the NAPFD average and reduced its variance for the majority of the experiments.

In figure \ref{fig:lambdamart-10-tff} we can see the comparison of distributions of \emph{time to the first failure},
for the metric \texttt{DCG@10} and 20 regression trees. For this configuration, the \texttt{CP-NCI} dataset,
which performed better across configurations, yields an average NAPFD value of 86\%.

The resulting induced selections are:
\begin{itemize}
    \item \texttt{S-SEL}: A safe selection was achieved with a selection size of 40\%, corresponding to 9\% of the execution time.
    \item \texttt{80-SEL}: A selection with average inclusiveness over 80\% was achieved with a selection size of 40\%, corresponding to 9\% of the execution time.
    \item \texttt{50-SEL}: A selection with average inclusiveness over 50\% was achieved with a selection size of 10\%, corresponding to 2\% of the execution time.
\end{itemize}

\begin{figure}
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:lambdamart-13-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/lambdamart-13/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the LambdaMART algorithm using the \texttt{MAP} training metric and 30 trees.}}
    \end{minipage}%
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:lambdamart-01-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/lambdamart-01/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the LambdaMART algorithm using the \texttt{NDCG@10} training metric and 30 trees.}}
    \end{minipage}%
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:lambdamart-02-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/lambdamart-02/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the LambdaMART algorithm using the \texttt{NDCG@10} training metric and 20 trees.}}
    \end{minipage}%
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:lambdamart-09-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/lambdamart-09/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the LambdaMART algorithm using the \texttt{DCG@10} training metric and 30 trees.}}
    \end{minipage}%
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:lambdamart-10-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/lambdamart-10/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the LambdaMART algorithm using the \texttt{DCG@10} training metric and 20 trees.}}
    \end{minipage}%
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:lambdamart-17-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/lambdamart-17/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the LambdaMART algorithm using the \texttt{NDCG@20} training metric and 30 trees.}}
    \end{minipage}%
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:lambdamart-18-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/lambdamart-18/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the LambdaMART algorithm using the \texttt{NDCG@20} training metric and 20 trees.}}
    \end{minipage}%
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:lambdamart-21-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/lambdamart-21/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the LambdaMART algorithm using the \texttt{NDCG@30} training metric and 30 trees.}}
    \end{minipage}%
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:lambdamart-22-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/lambdamart-22/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the LambdaMART algorithm using the \texttt{NDCG@30} training metric and 20 trees.}}
    \end{minipage}%
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:lambdamart-10-tff}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/lambdamart-10/distribution-comparison-TimeToFirstFailure.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of \emph{time to the first failure} values for the LambdaMART algorithm using the \texttt{DCG@10} training metric and 20 trees.}}
    \end{minipage}%
\end{figure}

\section{Results for MART}
The results show no significant impact on the training metric used for this algorithm. The best configuration uses 30 regression trees.

As with the other algorithms, the dataset that provided the best results is the one using coverage information for prioritizing, but
not using coverage information on the feature vectors.

For one of the best performing configurations, we can see in figure \ref{fig:mart-09-tff} a distribution of
the \emph{time to the first failure} values. The best configurations achieved an average NAPFD value of 67\%.

For such configuration, the resulting induced selection has similar values as the other algorithms:
The resulting induced selections are:
\begin{itemize}
    \item \texttt{S-SEL}: A safe selection was achieved with a selection size of 40\%, corresponding to 9\% of the execution time.
    \item \texttt{80-SEL}: A selection with average inclusiveness over 80\% was achieved with a selection size of 40\%, corresponding to 9\% of the execution time.
    \item \texttt{50-SEL}: A selection with average inclusiveness over 50\% was achieved with a selection size of 10\%, corresponding to 2\% of the execution time.
\end{itemize}

\begin{figure}
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:mart-09-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/mart-09/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the MART algorithm using the \texttt{DCG@10} training metric and 30 trees.}}
    \end{minipage}%
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:mart-13-napfd}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/mart-13/distribution-comparison-Selection-100-NAPFD.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of NAPFD values for the MART algorithm using the \texttt{MAP} training metric and 30 trees.}}
    \end{minipage}%
\end{figure}
\begin{figure}
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \label{fig:mart-09-tff}
        \includegraphics[width=0.9\textwidth]{data/evaluation/comparing-ranking-configurations/mart-09/distribution-comparison-TimeToFirstFailure.png}
        \parbox{0.9\textwidth}{\caption{Distribution across the different datasets of \emph{time to the first failure} values for the MART algorithm using the \texttt{DCG@10} training metric and 30 trees.}}
    \end{minipage}%
\end{figure}

\section{Other algorithms considered}

We performed training of AdaRank, with 500 rounds. As with other approaches, we considered different training metrics. 
However, none of the resulting models converged for our dataset.

Training of RankBoost was also attempted. However, as explained in section \ref{s:bg-tsp-rankboost},
it requires at each stage to keep a distribution $D$ of memory complexity $O(n^2)$ for $n$ the number of tests
on each CI job. With the dataset used, such memory complexity exceeded the memory capacity of even the HPC nodes 
where the training was performed.