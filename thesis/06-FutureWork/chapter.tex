\chapter{Future Work}\label{s:future}

\section{Usage of a prioritization technique in the context of the existing system.}

This work focused on an offline evaluation of the techniques. For an online implementation,
other technical challenges and practical considerations remain.

So far in our discussion, we did not tie how proposing a prioritization
with these techniques relates to how they are executed by the DME system, which is explained
in section \ref{s:bc-ci-dme}.

In this section, we outline the required changes, and possible strategies to use
such rankings.

Recall that the DME system, executes tasks from a given job, by
traversing each of the required dependencies defined by the model of the job. 
A single task may execute multiple test codeunits or test solutions. 

Given a ranking proposed by these techniques, we can use the induced selection
algorithm to reduce the number of test codeunits that each of these tasks executes.

Furthermore, if the selection results in \emph{job tasks} with no test codeunits to
execute, we can remove this task, along with the dependencies that were 
only required by this task. By doing so recursively we can remove entire paths of the
job's execution.

For clarity, see figure \ref{f:conc-fut-dag-removingtask}, where a DAG representing the dependencies of a job's 
execution is given. Nodes filled with black are the test tasks that after the selection
algorithm, had no tests to execute. Nodes filled with gray are the tasks that were only
required for such tests, and therefore could be removed.

\begin{figure}
    \centering
    \def\svgwidth{0.5\columnwidth}
    \includesvg[inkscapelatex=false]{thesis/figures/network-plots/removing-tasks-model}
    \caption{Tasks and predecessors removed from the job model.}
    \label{f:conc-fut-dag-removingtask}
\end{figure}

This would be an effective use of the selection proposed, however, engineering is required
to allow for such dynamic changes in a job's execution. 

Since test tasks can be performed in parallel by the DME system, a 
proposed prioritization of all the tests in the job can not be executed as given. 
Instead, the prioritization can be used locally on each task. For a given
test task and the overall prioritization, one can get a local prioritization for the
tests belonging to such task. However, the engineering that allows dynamically 
sorting the test codeunits in a single task is also missing.

Finally, the current build system allows for assigning priorities to the tasks to run,
which is taken into consideration when deciding which task to execute next from the 
set of tasks with completed dependencies. This could be dynamically assigned based on 
the prioritization of the tests being run. 

The main engineering challenge to allow for the usage of these proposals is to make
tests \emph{first-class citizens} of the data model proposed by the DME system.
Currently, the DME system has no knowledge of the tests being run by a task, as
it is instead the responsibility of the task's implementation.

\section{Tackling CI optimization from other angles}
\emph{Business Central} has several CI pipelines that constitute each of the different
development cycles, from different areas that the product has. We studied a single
optimization strategy for one of these CI pipelines, namely Test Selection and Prioritization.

However, different strategies exist in the literature, for instance, the usage of
test suite minimization to remove superfluous tests from a codebase.

Another approach could be to have learning models to predict the task failure
of each of the tasks a job is comprised from. A similar feature vector as
obtained for this project representing the change could be part of the training 
data used to train binary classifiers.

The approach taken on this work, to obtain a meaningful representation of the
changes done to a codebase can be further leveraged for future analysis of
this CI pipeline.

\section{Further evaluation of test prioritization techniques}\label{s:future-evalp}
Reducing the number of tests considered for each job execution for our training
dataset would allow for a larger timespan to be considered in our evaluation.
As each job can become more manageable for the learning algorithms and feature
extraction infrastructure. 

For instance, in this project, we were not able
to perform training of RankBoost, as its memory complexity is squared in the
number of tests.

This would also increase the confidence and validity of the results presented in this 
project. Having a larger dataset allows for multiple validation datasets, to increase the
confidence of the proposed selection sizes in this work.

Other approaches that avoid retraining like Reinforcement Learning could be 
leveraged and be the subject of future work with the produced dataset, although
comparison in existing literature favors \emph{Learning to Rank} approaches \cite{Bertolino2020LearningtoRankVR}.

In regards to \emph{Learning to Rank} approaches, more properties can be
obtained to give a vector representation of a codebase change and tests. 
For instance, in \cite{Bertolino2020LearningtoRankVR}, Bertolino, et. al. propose more 
code metrics that were not in scope for this project. Examples include 
cyclomatic complexity, number of public or private methods, number of 
new functions, and others. Other interesting features were explored by Busjaeger et. al. in \cite{Busjaeger2016LearningFT}, 
where besides coverage score, they use \emph{text similarity} metrics between the 
names of the test procedures and paths of the files or their content. 

Adding more useful features to this dataset would allow for more
robust criteria for the learning algorithms.